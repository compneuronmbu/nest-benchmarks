#!/bin/bash -x

# Give your job a name, so you can recognize it in the queue overview
#SBATCH --job-name=nest-hpc

# Each node has 16 or 20 CPU cores.
#SBATCH --nodes={{NODES}}
#SBATH --ntasks-per-node={{TASKS_PER_NODE}}
#SBATCH --cpus-per-task={{CPUS_PER_TASK}}

# This is a hard cap meaning that if the job runs longer than what is written
# here, it will be force-stopped by the server.
#              d-hh:mm:ss
#SBATCH --time=0-{{WALLTIME}}

# Define the partition on which the job shall run. May be omitted.
##SBATCH --partition normal

# How much memory you need.
# --mem will define memory per node and
# --mem-per-cpu will define memory per CPU/core. Choose one of those.
##SBATCH --mem-per-cpu=1500MB
#             31GB
#SBATCH --mem={{MEM}}

# Turn on mail notification. There are many possible self-explaining values:
# NONE, BEGIN, END, FAIL, ALL (including all aforementioned)
# For more values, check "man sbatch"
#SBATCH --mail-type=END,FAIL

# You may not place any commands before the last SBATCH directive
# Define and create a unique scratch directory for this job
SCRATCH_DIRECTORY=/global/work/${USER}/${SLURM_JOBID}.stallo-adm.uit.no
mkdir -p ${SCRATCH_DIRECTORY}
cd ${SCRATCH_DIRECTORY}

# You can copy everything you need to the scratch directory
# ${SLURM_SUBMIT_DIR} points to the path where this script was submitted from
cp -r /home/${USER}/NEST/bld_master ${SCRATCH_DIRECTORY}/bld
cp /home/${USER}/benchmarks/scripts/hpc_benchmark.sli ${SCRATCH_DIRECTORY}

# This is where the actual work is done.
cd $SCRATCH_DIRECTORY

bld/install/bin/nest --userargs={{SCALE}},{{CPUS_PER_TASK}},{{SIMTIME}},1000,true,1.5,1.5 hpc_benchmark.sli

# After the job is done we copy our output back to $SLURM_SUBMIT_DIR
#cp ${SCRATCH_DIRECTORY}/my_output ${SLURM_SUBMIT_DIR}

# In addition to the copied files, you will also find a file called
# slurm-1234.out in the submit directory. This file will contain all output that
# was produced during runtime, i.e. stdout and stderr.

# After everything is saved to the home directory, delete the work directory to
# save space on /global/work
cd ${SLURM_SUBMIT_DIR}
rm -rf ${SCRATCH_DIRECTORY}

# Finish the script
touch {{READY}}
exit 0
